# ABOUT additional bin/sample analyses
Copyright Jackson M. Tsuji, Neufeld Research Group, 2019
Part of the larger *IISD-ELA Chlorobia cyc2 project*.

All code here is to be run in a Bash terminal of a unix system (e.g., Ubuntu 16 or 18) unless indicated otherwise.

## Define where you downloaded the Github repo:
```
github_repo_location="/Analysis/jmtsuji/Chlorobia_cyc2_code"
```

## Software prerequisites
- miniconda (miniconda3 preferred)

## Gathered together the relevant genome bins
You have two options here:
1. Download the pre-made bin set in the Zenodo repository (recommended)
```
destination_dir="${github_repo_location}/Data_analysis_pipeline/05_bin_analysis/01_all_genome_bins"
zenodo_url= # TODO

mkdir -p ${destination_dir}
cd ${destination_dir}

# Download
wget ${zenodo_url}
# Downloads `dereplicated_genomes.tar.gz`
tar -xzf dereplicated_genomes.tar.gz
# Creates `dereplicated_genomes`
rm dereplicated_genomes.tar.gz

# Now move the bins all into the main folder
find dereplicated_genomes -type f -name "*.fa" | xargs -I {} mv {} .
rm -r dereplicated_genomes
```

2. Use the bins generated by the previous steps in the `Data_analysis_pipeline` run on your server (less common)
```
destination_dir="${github_repo_location}/Data_analysis_pipeline/05_bin_analysis/01_all_genome_bins"

mkdir -p ${destination_dir}
cd ${destination_dir}

# Note: need to change extension to ".fa" to work with one of the downstream scripts

# Get bins from the lake metagenomes
source_dir="${github_repo_location}/Data_analysis_pipeline/03_bin_curation/01_dereplication/output/dereplicated_genomes"
fasta_files=($(find ${source_dir} -name "*.fasta" | sort -h))

for fasta_file in ${fasta_files[@]}; do
    base_name=${fasta_file%.fasta}
    base_name=${base_name##*/}
    echo "[ $(date -u) ]: Linking '${base_name}'"
    ln -s ${fasta_file} ${base_name}.fa
done

# Get bins from the enrichment culture metagenomes
source_dir="${github_repo_location}/Data_analysis_pipeline/02_assembly_and_binning/enrichment_metagenomes"
fasta_files=($(find ${source_dir} -name "*.fasta" | grep "genomic_bins" | grep -v "checkm" | sort -h))

for fasta_file in ${fasta_files[@]}; do
    base_name=${fasta_file%.fasta}
    base_name=${base_name##*/}
    echo "[ $(date -u) ]: Linking '${base_name}'"
    ln -s ${fasta_file} ${base_name}.fa
done

# Get the curated bins
source_dir="${github_repo_location}/Data_analysis_pipeline/03_bin_curation/03_contig_ordering/ordered_genomes/final"
fna_files=($(find ${source_dir} -name "*.fna" | sort -h))

for fna_file in ${fna_files[@]}; do
    base_name=${fna_file%.fna}
    base_name=${base_name##*/}
    echo "[ $(date -u) ]: Linking '${base_name}'"
    ln -s ${fna_file} ${base_name}.fa
done

# Now MANUALLY delete the non-curated versions of the curated bins that were also copied!
```
All genome bins should now be in the `${github_repo_location}/Data_analysis_pipeline/05_bin_analysis/01_all_genome_bins` folder and have the extension `.fa`.


## Taxonomic classification via GTDBTk
All genome bins were taxonomically classified using the Genome Tree Database Toolkit.

Downloaded the database (only need to do the first time)
```bash
database_dir="${github_repo_location}/Data_analysis_pipeline/05_bin_analysis/02_gtdbtk_taxonomy/database"
mkdir -p ${download_dir}

wget -P ${database_dir} https://data.ace.uq.edu.au/public/gtdbtk/release_86/gtdbtk_r86_data.tar.gz
tar -xzf ${database_dir}/gtdbtk_r86_data.tar.gz -C ${database_dir}
rm ${database_dir}/gtdbtk_r86_data.tar.gz
# Database is now stored at "${database_dir}/release86"

# TODO
```

Install GTDBTk `v0.1.3`  
This is done semi-manually. Note that a conda install now exists to do this more automatically.
```
conda create -y -n gtdbtk_0.1.3 -c anaconda -c conda-forge -c bioconda -c biocore python=2.7 \
jinja2 scipy numpy mpld3 matplotlib dendropy \
prodigal pplacer fasttree hmmer fastani \
perl-bioperl perl-moose perl-ipc-run

# Then install gtdbtk via pip
conda activate gtdbtk_0.1.3
pip install --upgrade pip
pip install biolib
pip install gtdbtk=0.1.3

# Set the reference database path in the gtdbtk config file
config_dir="${CONDA_PREFIX}/lib/python2.7/site-packages/gtdbtk/config"
database_dir="${github_repo_location}/Data_analysis_pipeline/05_bin_analysis/02_gtdbtk_taxonomy/database"
line_to_modify=6
replacement="GENERIC_PATH = '${database_dir}/release86/'"
sed "${line_to_modify}s:.*:${replacement}:" ${config_dir}/config_template.py > ${config_dir}/config.py
```
Use this conda environment via `conda activate gtdbtk_0.1.3`

Then run the classifier (needs ~100 GB RAM!)
```
# Set user variables
input_dir="${github_repo_location}/Data_analysis_pipeline/05_bin_analysis/01_all_genome_bins"
output_dir="${github_repo_location}/Data_analysis_pipeline/05_bin_analysis/02_gtdbtk_taxonomy"
threads=12

gtdbtk classify_wf --genome_dir ${input_dir} --out_dir ${output_dir} -x fa --min_perc_aa 0 --prefix ELA111314_dRep_gtdbtk --cpus ${threads}
```
The output files `ELA111314_dRep_gtdbtk.ar122.summary.tsv` and `ELA111314_dRep_gtdbtk.bac120.summary.tsv` will be used downstream in Figure 3.

Note: in the actual analysis, GTDBTk was run separately for the lake vs. enrichment culture metagenomes, and the output files were then concatenated. However, running them together is simpler and I think should not change the results much at all.

## Custom gene scanning via MetAnnotate
Several genes of interest were searched for in the entire binned genome set:
- *rpoB* - housekeeping gene (RNA polymerase gene) useful as a taxonomic marker
- *cyc2* - candidate gene marker for iron oxidation used in this study
- *dsrA* - a gene marker for sulfide oxidation or dissimilatory sulfate reduction

HMMs for *rpoB* and *dsrA* were downloaded from the FunGene website:
```
download_dir="${github_repo_location}/Data_analysis_pipeline/05_bin_analysis/03_metannotate/hmm_files/genome_bins"
mkdir -p ${download_dir}
cd ${download_dir}

## Nov 2009 versions (# TODO - I don't see a way to specify the version number, so there is a chance these will be replaced by newer versions in the future)
# dsrA.hmm
curl -LOJ http://fungene.cme.msu.edu/hmm_download.spr?hmm_id=8

# rpoB.hmm
curl -LOJ http://fungene.cme.msu.edu/hmm_download.spr?hmm_id=31
```

The *cyc2* HMM was developed in this study and is available in Supplementary File S2 (`cyc2_all_references.hmm`; see also the `04_HMM_development` folder in `Data_analysis_pipeline`).
```
download_dir="${github_repo_location}/Data_analysis_pipeline/05_bin_analysis/03_metannotate/hmm_files/genome_bins"
mkdir -p ${download_dir}
cd ${download_dir}

cp "${github_repo_location}/Other/File_S2/cyc2_all_references.hmm" .
```

Predicted amino acid sequences of all genome bins using prokka `v1.13.3`
```
input_dir="${github_repo_location}/Data_analysis_pipeline/05_bin_analysis/01_all_genome_bins"
output_dir="${github_repo_location}/Data_analysis_pipeline/05_bin_analysis/01_all_genome_bins/prokka"
threads=12

mkdir -p ${output_dir}
genome_bins=($(find ${input_dir} -maxdepth 1 -iname "*.fa" | sort -h))

for genome in ${genome_bins[@]}; do

    genome_base=${genome%.fa}
    genome_base=${genome_base##*/}

    # Clean up the name to remove any odd characters, to make more compatible downstream e.g., with MetAnnotate
    genome_base=$(echo ${genome_base} | awk '{ gsub("[^A-Za-z0-9]", "_"); print }')

    echo ${genome_base}
    prokka --outdir ${work_dir}/prokka/${genome_base} --prefix ${genome_base} --locustag ${genome_base}_ --cpus ${threads} ${genome} 2&>/dev/null
    # N.B., might have some problems with Archaea!!

done
```

Downloaded the RefSeq protein database  
**SKIP** this step if you've already downloaded the database before
```
# User variables
orf_dir="${github_repo_location}/Data_analysis_pipeline/05_bin_analysis/01_all_genome_bins/prokka"
hmm_dir="${github_repo_location}/Data_analysis_pipeline/05_bin_analysis/03_metannotate/hmm_files/genome_bins"
output_dir="${github_repo_location}/Data_analysis_pipeline/05_bin_analysis/03_metannotate/output_genome_bins"
download_dir="${github_repo_location}/Data_analysis_pipeline/05_bin_analysis/03_metannotate/database"

mkdir -p ${download_dir} ${output_dir}

# Enter the docker container (will auto-download if you don't have it installed)
enter-metannotate ${download_dir} ${orf_dir} ${hmm_dir} ${output_dir}

# Download the database (can take a while)
cd $METANNOTATE_DIR
sudo chown linuxbrew ../databases
bash refseq_installation.sh /home/linuxbrew/databases
sudo chown -R root:root ../databases
# The 'chown' commands temporarily make the output folder belong to the linuxbrew user inside the Docker container so that the user can run the Docker commands. Files are given back to you at the end.

# Leave the container
exit
```

Ran MetAnnotate on the predicted amino acid sequences
```
# User variables
# NOTE: CHANGE the database_dir path if you've already downloaded Refseq.fa somewhere else on your machine and want to use that instead
orf_dir="${github_repo_location}/Data_analysis_pipeline/05_bin_analysis/01_all_genome_bins/prokka"
hmm_dir="${github_repo_location}/Data_analysis_pipeline/05_bin_analysis/03_metannotate/hmm_files/genome_bins"
output_dir="${github_repo_location}/Data_analysis_pipeline/05_bin_analysis/03_metannotate/output_genome_bins"
download_dir="${github_repo_location}/Data_analysis_pipeline/05_bin_analysis/03_metannotate/database"

# Enter the docker container (will auto-download if you don't have it installed)
enter-metannotate ${download_dir} ${orf_dir} ${hmm_dir} ${output_dir}

threads=12

ref_UID=$(stat -c "%u" /home/linuxbrew/output)
sudo chown -R linuxbrew /home/linuxbrew/output
echo ${threads} > MetAnnotate/concurrency.txt # set the number of threads
metannotate-wrapper-docker sequence orf_files hmm_files 2>&1 | tee output/metannotate_wrapper_docker.log
sudo chown -R $ref_UID /home/linuxbrew/output
# The 'chown' commands temporarily make the output folder belong to the linuxbrew user inside the Docker container so that the user can run the Docker commands. Files are given back to you at the end.

# Leave the docker container
exit
```

The output file `all_annotations`[random_code]`.tsv` shows the hits of the HMMs on all genomes, along with the e-value score. This is used in the code for Figure 3, and a copy of the file is the Figure 3 folder.  

## Relative abundance profiling using read mapping
### First had to install (only for first use)
```
## Get the git repo
work_dir="${github_repo_location}/Data_analysis_pipeline/05_bin_analysis/04_read_mapping"
mkdir -p ${work_dir} && cd ${work_dir}

git clone https://github.com/jmtsuji/atlas-extensions.git
cd ${work_dir}/atlas-extensions
git checkout 5b14f12

## Make the conda env and print the versions
conda create -y -n bin_mapping_stats -c conda-forge -c bioconda -c r samtools bbmap r r-plyr r-dplyr r-getopt

## Finish install
conda activate bin_mapping_stats
cp calculate_bin_abundance_in_metagenome.sh calculate_coverage_stats.R aggregate_mapping_stats.R ${CONDA_PREFIX}

# Log the conda env contents
conda list -n bin_mapping_stats > conda_env.log
```

Then run the profiler
```
atlas_dir="${github_repo_location}/Data_analysis_pipeline/02_assembly_and_binning"
input_dir="${github_repo_location}/Data_analysis_pipeline/05_bin_analysis/01_all_genome_bins"
work_dir="${github_repo_location}/Data_analysis_pipeline/05_bin_analysis/04_read_mapping"
qc_link_dir=${work_dir}/QC_reads
threads=12
memory=10 # gigabytes

# Make links of all QC'ed reads
mkdir -p ${link_dir}
find ${atlas_dir} -mindepth 4 -maxdepth 4 -name "*_QC_*.fastq.gz" | sort -h | xargs -I {} ln -s {} ${qc_link_dir}

cd ${work_dir}
calculate_bin_abundance_in_metagenome.sh ${input_dir} ${qc_link_dir} output ${threads} ${memory} 2>&1 | tee calculate_bin_abundance_in_metagenome.log
# Started 180927 at ~2:52 AM EDT
```

## Relative abundance profiling of raw reads via MetAnnotate
This was done as a cross-comparison to the above code.

The same three HMMs as mentioned earlier for MetAnnotate were used, along with one more for reference: *bchL* - pigment synthesis gene.
```
download_dir="${github_repo_location}/Data_analysis_pipeline/05_bin_analysis/03_metannotate/hmm_files/raw_reads"
mkdir -p ${download_dir}
cd ${download_dir}

# Get the HMMs used for the genome bins
cp ../genome_bins/*.hmm .

# Get the bchL HMM
# This is a pain; you have to download the entire TIGRFAM database (~0.1 Gb) and then pull out the HMM of interest
# Using TIGRFAM release 15.0
mkdir -p ${download_dir}/../TIGRFAM_15_0
cd ${download_dir}/../TIGRFAM_15_0
wget -O - ftp://ftp.jcvi.org/pub/data/TIGRFAMs/TIGRFAMs_15.0_HMM.tar.gz | tar -xzf -
cp TIGR01281.HMM ../raw_reads/TIGR01281_bchL.hmm

cd ../raw_reads
rm -r ../TIGRFAM_15_0 # Or skip this step if you want to keep TIGRFAM on your server
```

This first requires FGS++ (commit `299cc18`) to be installed semi-manually
```
conda create -y -n FGS++ python=3
conda install -y -n FGS++ -c anaconda -c conda-forge meson ninja

# Get FGS++ source code
cd /tmp
git clone https://github.com/unipept/FragGeneScanPlusPlus.git
cd FragGeneScanPlusPlus
git checkout 299cc18

# Build
conda activate FGS++
meson build
ninja -C build

# Then copy built binaries to conda env
cp build/FGS++ ${CONDA_PREFIX}/bin
cp -r train ${CONDA_PREFIX}/bin

# Then deleted install folder.
cd ..
rm -rf FragGeneScanPlusPlus
```
Use via `conda activate FGS++`


Predict ORFs from raw metagenomic data
```
conda activate FGS++

raw_read_dir="${github_repo_location}/Data_analysis_pipeline/05_bin_analysis/04_read_mapping/QC_reads" # made above
orf_dir="${github_repo_location}/Data_analysis_pipeline/05_bin_analysis/03_metannotate/raw_read_ORFs"
train_file_dir=""${CONDA_PREFIX}/bin/train"
threads=12
memory=50000 # in megabytes

mkdir -p ${orf_dir}
cd ${orf_dir}

fastq_files=($(find ${raw_read_dir} -type f -name "*.fastq.gz" | sort -h))

for fastq_file in ${fastq_files[@]}; do

    name_base=${fastq_file%.fastq.gz}
    name_base=${name_base##*/}
    echo "[ $(date -u) ]: Predicting ORFs for '${name_base}'"

    seqtk seq -A ${fastq_file} | \
        FGS++ -s stdin -o ${orf_dir}/${name_base}.frag -w 0 -r ${train_file_dir} \
        -t illumina_10 -p ${threads} -m ${memory} 2>&1 | tee ${orf_dir}/${name_base}.frag.log

done
```

Then run Metannotate
```
# User variables
# NOTE: CHANGE the database_dir path if you've already downloaded Refseq.fa somewhere else on your machine and want to use that instead
# If you haven't downloaded Refseq.fa yet, see the earlier usage of MetAnnotate in this file for how to do so
orf_dir="${github_repo_location}/Data_analysis_pipeline/05_bin_analysis/03_metannotate/raw_read_ORFs"
hmm_dir="${github_repo_location}/Data_analysis_pipeline/05_bin_analysis/03_metannotate/hmm_files/raw_reads"
output_dir="${github_repo_location}/Data_analysis_pipeline/05_bin_analysis/03_metannotate/output_raw_reads"
download_dir="${github_repo_location}/Data_analysis_pipeline/05_bin_analysis/03_metannotate/database"

mkdir -p ${output_dir}
# Enter the docker container (will auto-download if you don't have it installed)
enter-metannotate ${download_dir} ${orf_dir} ${hmm_dir} ${output_dir}

threads=12

ref_UID=$(stat -c "%u" /home/linuxbrew/output)
sudo chown -R linuxbrew /home/linuxbrew/output
echo ${threads} > MetAnnotate/concurrency.txt # set the number of threads
metannotate-wrapper-docker sequence orf_files hmm_files 2>&1 | tee output/metannotate_wrapper_docker.log
sudo chown -R $ref_UID /home/linuxbrew/output
# The 'chown' commands temporarily make the output folder belong to the linuxbrew user inside the Docker container so that the user can run the Docker commands. Files are given back to you at the end.

# Leave the docker container
exit
```

The output file `all_annotations`[random_code]`.tsv` shows the hits of the HMMs on all datsets, along with the e-value score. This is used in the code for Supplementary Figure S3.  
This file is a bit large, so a copy of the file is available in a separate Zenodo repository. See the Supplementary Figure S3 for download details.

