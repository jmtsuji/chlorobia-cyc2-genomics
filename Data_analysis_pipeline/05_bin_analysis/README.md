# ABOUT additional bin/sample analyses
Copyright Jackson M. Tsuji, Neufeld Research Group, 2020
Part of the larger *IISD-ELA Chlorobia cyc2 project*.

All code here is to be run in a Bash terminal of a unix system (e.g., Ubuntu 16 or 18) unless indicated otherwise.

## Define where you downloaded the Github repo:
```bash
github_repo_location="/Analysis/jmtsuji/chlorobia-cyc2-genomics"
```

## Software prerequisites
- miniconda (miniconda3 preferred)

## Gathered together the relevant genome bins
You have two options here:
**Option 1.** Download the pre-made bin set in the Zenodo repository (recommended)
```bash
destination_dir="${github_repo_location}/Data_analysis_pipeline/05_bin_analysis/01_all_genome_bins"
zenodo_url="https://zenodo.org/record/3228469/files/dereplicated_genomes.tar.gz"

mkdir -p ${destination_dir}
cd ${destination_dir}

# Download
wget -O - ${zenodo_url} | tar -xzf -
# Downloads `dereplicated_genomes.tar.gz` and unpacks into `dereplicated_genomes`

# Now move the bins all into the main folder
find dereplicated_genomes -type f -name "*.fa" | xargs -I {} mv {} .
rm -r dereplicated_genomes
```

**Option 2.** Use the bins generated by the previous steps in the `Data_analysis_pipeline` run on your server (less common)
```bash
destination_dir="${github_repo_location}/Data_analysis_pipeline/05_bin_analysis/01_all_genome_bins"

mkdir -p ${destination_dir}
cd ${destination_dir}

# Note: need to change extension to ".fa" to work with one of the downstream scripts

# Get bins from the lake metagenomes
source_dir="${github_repo_location}/Data_analysis_pipeline/03_bin_curation/01_dereplication/output/dereplicated_genomes"
fasta_files=($(find ${source_dir} -name "*.fasta" | sort -h))

for fasta_file in ${fasta_files[@]}; do
    base_name=${fasta_file%.fasta}
    base_name=${base_name##*/}
    echo "[ $(date -u) ]: Linking '${base_name}'"
    ln -s ${fasta_file} ${base_name}.fa
done

# Get bins from the enrichment culture metagenomes
source_dir="${github_repo_location}/Data_analysis_pipeline/02_assembly_and_binning/enrichment_metagenomes"
fasta_files=($(find ${source_dir} -name "*.fasta" | grep "genomic_bins" | grep -v "checkm" | sort -h))

for fasta_file in ${fasta_files[@]}; do
    base_name=${fasta_file%.fasta}
    base_name=${base_name##*/}
    echo "[ $(date -u) ]: Linking '${base_name}'"
    ln -s ${fasta_file} ${base_name}.fa
done

# Get the curated bins
source_dir="${github_repo_location}/Data_analysis_pipeline/03_bin_curation/03_contig_ordering/ordered_genomes/final"
fna_files=($(find ${source_dir} -name "*.fna" | sort -h))

for fna_file in ${fna_files[@]}; do
    base_name=${fna_file%.fna}
    base_name=${base_name##*/}
    echo "[ $(date -u) ]: Linking '${base_name}'"
    ln -s ${fna_file} ${base_name}.fa
done

# Now MANUALLY delete the non-curated versions of the curated bins that were also copied!
```
All genome bins should now be in the `${github_repo_location}/Data_analysis_pipeline/05_bin_analysis/01_all_genome_bins` folder and have the extension `.fa`.


## Taxonomic classification via GTDBTk
All genome bins were taxonomically classified using the Genome Tree Database Toolkit.

Downloaded the database (only need to do the first time)  
Release 86, version 3
```bash
database_dir="${github_repo_location}/Data_analysis_pipeline/05_bin_analysis/02_gtdbtk_taxonomy/database"
mkdir -p ${download_dir}

# Download the DB
wget -O - https://data.ace.uq.edu.au/public/gtdbtk/release_86/gtdbtk.r86_v3_data.tar.gz | tar -xzf -
# Makes the `release86` directory
mv release86/* .
rm -r release86
```

Install GTDBTk `v0.2.2`  
```bash
# Create the conda env
conda create -n gtdbtk_0.2.2 -c bioconda gtdbtk=0.2.2

# Then symlink the DB contents to the conda env
conda activate gtdbtk_0.2.2
db_filepaths=($(find ${database_dir} -maxdepth 1 | sort -h | tail -n +2))

for db_filepath in ${db_filepaths[@]}; do
    echo ${db_filepath##*/}
    ln -s ${db_filepath} ${CONDA_PREFIX}/share/gtdbtk-0.2.2/db
done
# Now GTDB will be able to find the DB automatically
```
Use this environment via `conda activate gtdbtk_0.2.2`

You can then test the install if you'd like:
```bash
test_dir="${github_repo_location}/Data_analysis_pipeline/05_bin_analysis/02_gtdbtk_taxonomy/gtdbtk_test"
threads=10

mkdir -p ${test_dir}
cd ${test_dir}
gtdbtk test --out_dir . --cpus ${threads}
```

Then run the classifier (needs ~100 GB RAM!)
```bash
# Set user variables
input_dir="${github_repo_location}/Data_analysis_pipeline/05_bin_analysis/01_all_genome_bins"
output_dir="${github_repo_location}/Data_analysis_pipeline/05_bin_analysis/02_gtdbtk_taxonomy"
threads=10

mkdir -p ${output_dir}
cd ${output_dir}

gtdbtk classify_wf --genome_dir ${input_dir} --out_dir ${output_dir} -x fa --min_perc_aa 10 --prefix ELA111314_dRep_gtdbtk --cpus ${threads} --debug
```
The output files `ELA111314_dRep_gtdbtk.ar122.summary.tsv` and `ELA111314_dRep_gtdbtk.bac120.summary.tsv` will be used downstream in Figure 3.  
By using the `--debug` flag, I'm able to keep the ORF predictions around to use for the next step below.

## Custom gene scanning via MetAnnotate
Several genes of interest were searched for in the entire binned genome set:
- *rpoB* - housekeeping gene (RNA polymerase gene) useful as a taxonomic marker
- *cyc2* - candidate gene marker for iron oxidation used in this study
- *dsrA* - a gene marker for sulfide oxidation or dissimilatory sulfate reduction

HMMs for *rpoB* and *dsrA* were downloaded from the FunGene website:
```bash
download_dir="${github_repo_location}/Data_analysis_pipeline/05_bin_analysis/03_metannotate/genome_bins/hmm_files"
mkdir -p ${download_dir}
cd ${download_dir}

## Nov 2009 versions (# TODO - I don't see a way to specify the version number, so there is a chance these will be replaced by newer versions in the future)
# dsrA.hmm
curl -LOJ http://fungene.cme.msu.edu/hmm_download.spr?hmm_id=8

# rpoB.hmm
curl -LOJ http://fungene.cme.msu.edu/hmm_download.spr?hmm_id=31
```

The *cyc2* HMM was developed in this study and is available in Supplementary File 2. 
(That HMM is named `cyc2_all_references.hmm` in the `04_HMM_development` folder in `Data_analysis_pipeline`). 
Although only that general version of the *cyc2* HMM was used in the paper, the HMM development folder also contains a 
*Chlorobia*-specific version of the *cyc2* HMM for interest.
```bash
cd ${download_dir}

cp "${github_repo_location}/Data_analysis_pipeline/04_HMM_development/04_HMMs/cyc2_all_references.hmm" .
cp "${github_repo_location}/Data_analysis_pipeline/04_HMM_development/04_HMMs/cyc2_Chlorobia.hmm" .
```

Got prodigal `v2.6.3` amino acid predictions from the GTDBTk intermediate files (from the above GTDBTk step)
```bash
input_dir="${github_repo_location}/Data_analysis_pipeline/05_bin_analysis/02_gtdbtk_taxonomy/marker_genes"
output_dir="${github_repo_location}/Data_analysis_pipeline/05_bin_analysis/03_metannotate/genome_bins/orf_files"

mkdir -p ${output_dir}
cd ${output_dir}

genome_bin_dirs=($(find ${input_dir} -maxdepth 1 -mindepth 1 -type d | sort -h))
echo "[ $(date -u) ]: Linking ${#genome_bin_dirs[@]} genome bin ORF files"

for genome_bin_dir in ${genome_bin_dirs[@]}; do

    genome_basename=${genome_bin_dir##*/}

    # Clean up the name to remove any odd characters, to make more compatible downstream e.g., with MetAnnotate
    genome_basename_clean=$(echo ${genome_basename} | awk '{ gsub("[^A-Za-z0-9]", "_"); print }')

    # Hard link to the new directory
    echo "[ $(date -u) ]: Linking ${genome_basename}"
    ln ${genome_bin_dir}/${genome_basename}_protein.faa ${output_dir}/${genome_basename_clean}.faa

done

echo "[ $(date -u) ]: Linking finished."
```

Downloaded the RefSeq protein database  
**SKIP** this step if you've already downloaded the database before
```bash
# User variables
orf_dir="${github_repo_location}/Data_analysis_pipeline/05_bin_analysis/03_metannotate/genome_bins/orf_files"
hmm_dir="${github_repo_location}/Data_analysis_pipeline/05_bin_analysis/03_metannotate/genome_bins/hmm_files"
output_dir="${github_repo_location}/Data_analysis_pipeline/05_bin_analysis/03_metannotate/genome_bins/metannotate"
download_dir="${github_repo_location}/Data_analysis_pipeline/05_bin_analysis/03_metannotate/database"

mkdir -p ${download_dir} ${output_dir}

# Enter the docker container (will auto-download if you don't have it installed)
enter-metannotate ${download_dir} ${orf_dir} ${hmm_dir} ${output_dir}

# Download the database (can take a while)
cd $METANNOTATE_DIR
sudo chown linuxbrew ../databases
bash refseq_installation.sh /home/linuxbrew/databases
sudo chown -R root:root ../databases
# The 'chown' commands temporarily make the output folder belong to the linuxbrew user inside the Docker container so that the user can run the Docker commands. Files are given back to you at the end.

# Leave the container
exit
```

Ran MetAnnotate on the predicted amino acid sequences
```bash
# User variables
# NOTE: CHANGE the database_dir path if you've already downloaded Refseq.fa somewhere else on your machine and want to use that instead
orf_dir="${github_repo_location}/Data_analysis_pipeline/05_bin_analysis/03_metannotate/genome_bins/orf_files"
hmm_dir="${github_repo_location}/Data_analysis_pipeline/05_bin_analysis/03_metannotate/genome_bins/hmm_files"
output_dir="${github_repo_location}/Data_analysis_pipeline/05_bin_analysis/03_metannotate/genome_bins/metannotate"
download_dir="${github_repo_location}/Data_analysis_pipeline/05_bin_analysis/03_metannotate/database"

mkdir -p ${output_dir}
cd ${output_dir}

# Enter the docker container (will auto-download if you don't have it installed)
enter-metannotate ${download_dir} ${orf_dir} ${hmm_dir} ${output_dir}

threads=10

ref_UID=$(stat -c "%u" /home/linuxbrew/output)
sudo chown -R linuxbrew /home/linuxbrew/output
echo ${threads} > MetAnnotate/concurrency.txt # set the number of threads
metannotate-wrapper-docker sequence orf_files hmm_files 2>&1 | tee output/metannotate_wrapper_docker.log
sudo chown -R $ref_UID /home/linuxbrew/output
# The 'chown' commands temporarily make the output folder belong to the linuxbrew user inside the Docker container so that the user can run the Docker commands. Files are given back to you at the end.

# Leave the docker container
exit
```

The output file `all_annotations`[random_code]`.tsv` shows the hits of the HMMs on all genomes, along with the e-value score. It was renamed to `metannotate_annotations_genome_bins.tsv` for clarity in this folder. This output file is used in the code for Figure 3 -- see the Figure 3 folder.

Note that MetAnnotate hung on the final annotation combining step on my machine, so I had to manually merge the annotation files from each individual HMM to make `metannotate_annotations_genome_bins.tsv`:
```
cd "${github_repo_location}/Data_analysis_pipeline/05_bin_analysis/03_metannotate/genome_bins/metannotate"
output_filename="metannotate_annotations_genome_bins.tsv"
hmm_names=(cyc2_Chlorobia cyc2_all_references dsrA rpoB)

# Initialize output file
# TODO - add similar check here to below (although if the match fails, it will still be picked up below...)
head -n 1 ${hmm_names[0]}*_all_annotations*.tsv > ${output_filename}

for hmm_name in ${hmm_names[@]}; do

    echo "[ $(date -u) ]: Adding annotations from '${hmm_name}'"

    # Confirm only one file matches the pattern
    if [ $(find . -name "${hmm_name}*_all_annotations*.tsv" | wc -l) != 1 ]; then

        echo "[ $(date -u) ]: ERROR: found multiple files matching the pattern '${hmm_name}*_all_annotations*.tsv'. Exiting..."
        exit 1

    fi

    # Merge
    tail -n +2 ${hmm_name}*_all_annotations*.tsv >> ${output_filename}

done
```

Note: *cyc2* genes were also detected in some non-*Chlorobia* genome bins using MetAnnotate. An aligned summary of all of those hits is available as `cyc2_Fig3_other_hits_L227_L442.faa` in the `03_metannotate/genome_bins` folder. They are presented in the bubble plot in Figure 3.

## Relative abundance profiling using read mapping
(This requires you to have run ATLAS. Otherwise, the output file from this analysis is provided in the repo.)  
First had to install (only for first use)
```bash
## Get the git repo
work_dir="${github_repo_location}/Data_analysis_pipeline/05_bin_analysis/04_read_mapping"
mkdir -p ${work_dir} && cd ${work_dir}

git clone https://github.com/jmtsuji/atlas-extensions.git
cd ${work_dir}/atlas-extensions
git checkout 5b14f12

## Make the conda env and print the versions
conda create -y -n bin_mapping_stats -c conda-forge -c bioconda -c r samtools bbmap r r-plyr r-dplyr r-getopt

## Finish install
conda activate bin_mapping_stats
cp calculate_bin_abundance_in_metagenome.sh calculate_coverage_stats.R aggregate_mapping_stats.R ${CONDA_PREFIX}

# Log the conda env contents
conda list -n bin_mapping_stats > conda_env.log
```

Then run the profiler
```bash
atlas_dir="${github_repo_location}/Data_analysis_pipeline/02_assembly_and_binning"
input_dir="${github_repo_location}/Data_analysis_pipeline/05_bin_analysis/01_all_genome_bins"
work_dir="${github_repo_location}/Data_analysis_pipeline/05_bin_analysis/04_read_mapping"
qc_link_dir=${work_dir}/QC_reads
threads=12
memory=10 # gigabytes

# Make links of all QC'ed reads
mkdir -p ${link_dir}
find ${atlas_dir} -mindepth 4 -maxdepth 4 -name "*_QC_*.fastq.gz" | sort -h | xargs -I {} ln -s {} ${qc_link_dir}

cd ${work_dir}
calculate_bin_abundance_in_metagenome.sh ${input_dir} ${qc_link_dir} output ${threads} ${memory} 2>&1 | tee calculate_bin_abundance_in_metagenome.log
# Started 180927 at ~2:52 AM EDT
```
See the main output file, `genome_bin_mapping_stats.tsv`, pre-included in the repo.

Also estimate the percentage of reads that assembled via the ATLAS read mapping files  
(This requires you to have run ATLAS. Otherwise, the output file from this analysis is provided in the repo.)

```bash
# Set variables
atlas_dir="${github_repo_location}/Data_analysis_pipeline/02_assembly_and_binning"
output_dir="${github_repo_location}/Data_analysis_pipeline/05_bin_analysis/04_read_mapping"
output_filepath="${output_dir}/assembled_read_stats.tsv"

# HARD-CODED locations of the ATLAS BAM files of interest for this analysis (it's tricky because of the co-assembly to do this automatically)
bam_files=("${atlas_dir}/lake_metagenomes/coassembly/CA-L227-2013/multi_mapping/L227-2013-6m.bam"
    "${atlas_dir}/lake_metagenomes/coassembly/CA-L227-2013/multi_mapping/L227-2013-8m.bam"
    "${atlas_dir}/lake_metagenomes/coassembly/CA-L227-2014/multi_mapping/L227-2014-6m.bam"
    "${atlas_dir}/lake_metagenomes/coassembly/CA-L227-2014/multi_mapping/L227-2014-8m.bam"
    "${atlas_dir}/lake_metagenomes/coassembly/CA-L442/multi_mapping/L442-2011-16-5m.bam"
    "${atlas_dir}/lake_metagenomes/coassembly/CA-L442/multi_mapping/L442-2014-15m.bam"
    "${atlas_dir}/enrichment_metagenomes/L227_S_6D/sequence_alignment/L227_S_6D.bam"
    "${atlas_dir}/enrichment_metagenomes/L304_S_6D/sequence_alignment/L304_S_6D.bam")

# Initialize the output file
mkdir -p ${output_dir}
cd ${output_dir}
printf "metagenome\ttotal_reads\tmapped_assembled_reads\n" > ${output_filepath}

# Count the mapped reads
for bam_file in ${bam_files[@]}; do

    filename_base=${bam_file%.bam}
    filename_base=${filename_base##*/}

    echo "[ $(date -u) ]: Counting '${bam_file##*/}'"
    printf "${filename_base}\t" >> ${output_filepath}

    # Get the total reads
    total_reads=$(samtools view -c ${bam_file} >> ${output_filepath})
    printf "${total_reads}\t" >> ${output_filepath}

    # Get the mapped reads
    mapped_reads=$(samtools view -c -F 4 ${bam_file} >> ${output_filepath})
    printf "${mapped_reads}\n" >> ${output_filepath}

done
echo "[ $(date -u) ]: Done."
```
See output file, `assembled_read_stats.tsv`, in the repo folder.

These output files are used in Figure 3 -- see the Figure 3 folder for details.

## Relative abundance profiling of targeted genes in raw read data via MetAnnotate
This was done as a cross-comparison to the above MetAnnotate code.

The same three HMMs as mentioned earlier for MetAnnotate were used, along with one more for reference: *bchL* - pigment synthesis gene.
```bash
download_dir="${github_repo_location}/Data_analysis_pipeline/05_bin_analysis/03_metannotate/unassembled_reads/hmm_files"
mkdir -p ${download_dir}
cd ${download_dir}

# Get the HMMs used for the genome bins
cp ../../genome_bins/hmm_files/*.hmm .

# Get the bchL HMM
# This is a pain; you have to download the entire TIGRFAM database (~0.1 Gb) and then pull out the HMM of interest
# Using TIGRFAM release 15.0
mkdir -p ${download_dir}/../TIGRFAM_15_0
cd ${download_dir}/../TIGRFAM_15_0
wget -O - ftp://ftp.jcvi.org/pub/data/TIGRFAMs/TIGRFAMs_15.0_HMM.tar.gz | tar -xzf -
cp TIGR01281.HMM ../hmm_files/TIGR01281_bchL.hmm

cd ../hmm_files
rm -r ../TIGRFAM_15_0 # Or skip this step if you want to keep TIGRFAM on your server
```

This first requires FGS++ (commit `299cc18`) to be installed semi-manually
```bash
conda create -y -n FGS++ python=3
conda install -y -n FGS++ -c anaconda -c conda-forge meson ninja

# Get FGS++ source code
cd /tmp
git clone https://github.com/unipept/FragGeneScanPlusPlus.git
cd FragGeneScanPlusPlus
git checkout 299cc18

# Build
conda activate FGS++
meson build
ninja -C build

# Then copy built binaries to conda env
cp build/FGS++ ${CONDA_PREFIX}/bin
cp -r train ${CONDA_PREFIX}/bin

# Then deleted install folder.
cd ..
rm -rf FragGeneScanPlusPlus
```
Use via `conda activate FGS++`


Predict ORFs from raw metagenomic data
```bash
conda activate FGS++

raw_read_dir="${github_repo_location}/Data_analysis_pipeline/05_bin_analysis/04_read_mapping/QC_reads" # made above
orf_dir="${github_repo_location}/Data_analysis_pipeline/05_bin_analysis/03_metannotate/unassembled_reads/orf_files"
train_file_dir=""${CONDA_PREFIX}/bin/train"
threads=12
memory=50000 # in megabytes

mkdir -p ${orf_dir}
cd ${orf_dir}

fastq_files=($(find ${raw_read_dir} -type f -name "*.fastq.gz" | sort -h))

for fastq_file in ${fastq_files[@]}; do

    name_base=${fastq_file%.fastq.gz}
    name_base=${name_base##*/}
    echo "[ $(date -u) ]: Predicting ORFs for '${name_base}'"

    seqtk seq -A ${fastq_file} | \
        FGS++ -s stdin -o ${orf_dir}/${name_base}.frag -w 0 -r ${train_file_dir} \
        -t illumina_10 -p ${threads} -m ${memory} 2>&1 | tee ${orf_dir}/${name_base}.frag.log

done
```

Then run Metannotate
```bash
# User variables
# NOTE: CHANGE the database_dir path if you've already downloaded Refseq.fa somewhere else on your machine and want to use that instead
# If you haven't downloaded Refseq.fa yet, see the earlier usage of MetAnnotate in this file for how to do so
orf_dir="${github_repo_location}/Data_analysis_pipeline/05_bin_analysis/03_metannotate/unassembled_reads/orf_files"
hmm_dir="${github_repo_location}/Data_analysis_pipeline/05_bin_analysis/03_metannotate/unassembled_reads/hmm_files"
output_dir="${github_repo_location}/Data_analysis_pipeline/05_bin_analysis/03_metannotate/unassembled_reads/metannotate"
download_dir="${github_repo_location}/Data_analysis_pipeline/05_bin_analysis/03_metannotate/database"

mkdir -p ${output_dir}
cd ${output_dir}

# Enter the docker container (will auto-download if you don't have it installed)
enter-metannotate ${download_dir} ${orf_dir} ${hmm_dir} ${output_dir}

threads=20

ref_UID=$(stat -c "%u" /home/linuxbrew/output)
sudo chown -R linuxbrew /home/linuxbrew/output
echo ${threads} > MetAnnotate/concurrency.txt # set the number of threads
metannotate-wrapper-docker sequence orf_files hmm_files 2>&1 | tee output/metannotate_wrapper_docker.log
sudo chown -R $ref_UID /home/linuxbrew/output
# The 'chown' commands temporarily make the output folder belong to the linuxbrew user inside the Docker container so that the user can run the Docker commands. Files are given back to you at the end.

# Leave the docker container
exit
```

The output file `all_annotations`[random_code]`.tsv` shows the hits of the HMMs on all datsets, along with the e-value score. This was renamed (and gzipped) to `metannotate_annotations_unassembled_reads.tsv.gz` for clarity. This file is used in the code for Supplementary Figure 7. Because of its size, it is available for download on Zenodo -- see the Supplementary Figure 7 folder.

## Searching for additional genes potentially involved in Fe and S cycling
Searched the entire genome bin set from this study to get a sense of the Fe/S cycling genomic potential in the lake metagenomes.

### Iron cycling genes
Installed FeGenie commit `30174bb` via a modified version of `FeGenie/setup.sh` included in the repo:
```bash
mkdir -p "${github_repo_location}/Data_analysis_pipeline/05_bin_analysis/05_other_Fe_S_genes/01_FeGenie"
cd "${github_repo_location}/Data_analysis_pipeline/05_bin_analysis/05_other_Fe_S_genes/01_FeGenie"

git clone https://github.com/Arkadiy-Garber/FeGenie.git
cd FeGenie
git checkout 30174bb

gzip -d test_dataset.tar.gz
tar xf test_dataset.tar
rm test_dataset.tar

## Create conda environment
conda create -y -c bioconda -c conda-forge -c au-eoed -n fegenie_30174bb python=3.7 hmmer diamond prodigal blast

## Activate environment
conda activate fegenie_30174bb

## Create directory for conda env-specific source files
mkdir -p ${CONDA_PREFIX}/etc/conda/activate.d

## Add FeGenie bin path and HMM_dir variable:
echo '#!/bin/sh'" \
export PATH=\"$(pwd):"'$PATH'\"" \
export rscripts=\"$(pwd)/rscripts\"
export iron_hmms=\"$(pwd)/hmms/iron\"" >> ${CONDA_PREFIX}/etc/conda/activate.d/env_vars.sh
## **HAVE TO MANUALLY ADD WHITESPACES AFTER RUNNING THIS CODE in ${CONDA_PREFIX}/etc/conda/activate.d/env_vars.sh

# Re-activate environment so variable and PATH changes take effect
conda activate fegenie
```

Minor code cleanup to get FeGenie to run:
- Edited line 120 of `FeGenie/hmms/iron/HMM-bitcutoffs.txt` to delete an extraneous space:
```
# OLD
PF16525-MHB 	27.3

# NEW
PF16525-MHB	27.3
```
- Edited line 479 of `FeGenie/FeGenie.py`
```python
# OLD - it tried to remove mainDir.txt even if the file wasn't actually generated by the tool
os.system("rm HMMlib.txt rscripts.txt mainDir.txt")

# NEW - only delete mainDir.txt if it was actually generated
# Added at line 472 inside the conditional:
os.system("rm mainDir.txt")
# Then changed the old line at 479 to:
os.system("rm HMMlib.txt rscripts.txt")
```
Now ready to go with FeGenie. Before using FeGenie, run `conda activate fegenie_30174bb`.

Searched for genes involved in Fe cycling using FeGenie
```bash
work_dir="${github_repo_location}/Data_analysis_pipeline/05_bin_analysis/05_other_Fe_S_genes/01_FeGenie"
bin_dir="${github_repo_location}/Data_analysis_pipeline/05_bin_analysis/05_other_Fe_S_genes/01_FeGenie"

mkdir -p "${work_dir}"
cd "${work_dir}"

FeGenie.py -bin_dir 01_all_genome_bins -bin_ext fa -t 16 -out output 2>&1 | tee FeGenie.log
```
The output file `FeGenie-geneSummary.csv` shows the Fe-cycling related gene hits (and bitscores etc.) for each genome bin. This file is used in the code for Supplementary Figure 8.

### S cycling genes
Got HMMs
```bash
hmm_dir="${github_repo_location}/Data_analysis_pipeline/05_bin_analysis/05_other_Fe_S_genes/02_S_genes/01_hmm_files"
tigrfam_dir="${hmm_dir}/TIGRFAM_15_0"

mkdir -p "${hmm_dir}"
cd "${hmm_dir}"

# Download HMMs from FunGene
curl -LOJ http://fungene.cme.msu.edu/hmm_download.spr?hmm_id=8 # dsrA.hmm; June 2009 version
curl -LOJ http://fungene.cme.msu.edu/hmm_download.spr?hmm_id=10 # dsrB.hmm; June 2009 version
curl -LOJ http://fungene.cme.msu.edu/hmm_download.spr?hmm_id=383 # Downloads soxB.hmm; March 2010 version

# Download HMMs from TIGRFAM release 15.0
# This is a pain; you have to download the entire TIGRFAM database (~0.1 Gb) and then pull out the HMM of interest
mkdir -p "${tigrfam_dir}"
cd "${tigrfam_dir}"
wget -O - ftp://ftp.jcvi.org/pub/data/TIGRFAMs/TIGRFAMs_15.0_HMM.tar.gz | tar -xzf -
cp "${tigrfam_dir}/TIGR02061.HMM" "aprA_TIGR02061.hmm"
cp "${tigrfam_dir}/TIGR02060.HMM" "aprB_TIGR02060.hmm"
cd ..
rm -r "${tigrfam_dir}" # Or skip this step if you want to keep TIGRFAM on your server
```

Got subject genome ORF predictions (linking from the GTDB predictions)
```bash
faa_dir="${github_repo_location}/Data_analysis_pipeline/05_bin_analysis/02_gtdbtk_taxonomy/marker_genes"
output_dir="${github_repo_location}/Data_analysis_pipeline/05_bin_analysis/05_other_Fe_S_genes/02_S_genes/02_subjects"

mkdir -p "${output_dir}"
cd "${output_dir}"

genome_bin_dirs=($(find "${input_dir}" -maxdepth 1 -mindepth 1 -type d | sort -h))
echo "[ $(date -u) ]: Linking ${#genome_bin_dirs[@]} genome bin ORF files"

for genome_bin_dir in ${genome_bin_dirs[@]}; do

    genome_basename=${genome_bin_dir##*/}

    # Clean up the name to remove any odd characters, to make more compatible downstream e.g., with MetAnnotate
    genome_basename_clean=$(echo ${genome_basename} | awk '{ gsub("[^A-Za-z0-9]", "_"); print }')

    # Hard link to the new directory
    echo "[ $(date -u) ]: Linking ${genome_basename}"
    ln ${genome_bin_dir}/${genome_basename}_protein.faa ${output_dir}/${genome_basename_clean}.faa

done

echo "[ $(date -u) ]: Linking finished."
```

Install hmmsearch
```bash
conda create -n hmmer -c bioconda hmmer=3.1b2
```
Before running hmmsearch, make sure to activate the conda env by running `conda activate hmmer`.

Ran hmmsearch on the subject genome ORF predictions
```bash
work_dir="${github_repo_location}/Data_analysis_pipeline/05_bin_analysis/05_other_Fe_S_genes/02_S_genes/03_hmmsearch"
output_filepath="${work_dir}/S_cycling_hmm_search_results_raw.tsv"
evalue=1e-40
threads=10

mkdir -p "${work_dir}"
cd "${work_dir}"

printf "genome\torf_id\thmm_id\tevalue\tscore\tbias\n" > "${output_filepath}"

genome_files=($(find ../02_subjects -type f -iname "*.faa" | sort -h))
hmm_files=($(find ../01_hmm_files -type f -iname "*.hmm" | sort -h))

for genome_file in ${genome_files[@]}; do
genome_basename="${genome_file##*/}"
genome_basename="${genome_basename%.faa}"
echo "[ $(date -u) ]: Scanning '${genome_basename}'"

for hmm_file in ${hmm_files[@]}; do
echo "[ $(date -u) ]: Running HMM '${hmm_file##*/}'"

# hmmsearch default table header:
#                                                               --- full sequence ---- --- best 1 domain ---- --- domain number estimation ----
# target name        accession  query name           accession    E-value  score  bias   E-value  score  bias   exp reg clu  ov env dom rep inc description of target

# hmmsearch version 3.1b2
hmmsearch --tblout /dev/stdout -o /dev/null -E ${evalue} --cpu ${threads} "${hmm_file}" "${genome_file}" | grep -v "^#" | tr -s ' ' $'\t' | cut -f 1,3,5-7 | sed "s|^|${genome_basename}\t|g" >> "${output_filepath}"

done
done
```
The output file `S_cycling_hmm_search_results_raw.tsv` shows the S-cycling related gene hits (and evalue etc.) for each genome bin. This file is used in the code for Supplementary Figure 8.

All done!
