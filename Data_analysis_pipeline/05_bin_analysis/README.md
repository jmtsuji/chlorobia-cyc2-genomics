# ABOUT additional bin/sample analyses
Copyright Jackson M. Tsuji, Neufeld Research Group, 2019
Part of the larger *IISD-ELA Chlorobia cyc2 project*.

All code here is to be run in a Bash terminal of a unix system (e.g., Ubuntu 16 or 18) unless indicated otherwise.

## Define where you downloaded the Github repo:
```bash
github_repo_location="/Analysis/jmtsuji/chlorobia-cyc2-genomics"
```

## Software prerequisites
- miniconda (miniconda3 preferred)

## Gathered together the relevant genome bins
You have two options here:
**Option 1.** Download the pre-made bin set in the Zenodo repository (recommended)
```bash
destination_dir="${github_repo_location}/Data_analysis_pipeline/05_bin_analysis/01_all_genome_bins"
zenodo_url="https://zenodo.org/record/3228469/files/dereplicated_genomes.tar.gz"

mkdir -p ${destination_dir}
cd ${destination_dir}

# Download
wget -O - ${zenodo_url} | tar -xzf -
# Downloads `dereplicated_genomes.tar.gz` and unpacks into `dereplicated_genomes`

# Now move the bins all into the main folder
find dereplicated_genomes -type f -name "*.fa" | xargs -I {} mv {} .
rm -r dereplicated_genomes
```

**Option 2.** Use the bins generated by the previous steps in the `Data_analysis_pipeline` run on your server (less common)
```bash
destination_dir="${github_repo_location}/Data_analysis_pipeline/05_bin_analysis/01_all_genome_bins"

mkdir -p ${destination_dir}
cd ${destination_dir}

# Note: need to change extension to ".fa" to work with one of the downstream scripts

# Get bins from the lake metagenomes
source_dir="${github_repo_location}/Data_analysis_pipeline/03_bin_curation/01_dereplication/output/dereplicated_genomes"
fasta_files=($(find ${source_dir} -name "*.fasta" | sort -h))

for fasta_file in ${fasta_files[@]}; do
    base_name=${fasta_file%.fasta}
    base_name=${base_name##*/}
    echo "[ $(date -u) ]: Linking '${base_name}'"
    ln -s ${fasta_file} ${base_name}.fa
done

# Get bins from the enrichment culture metagenomes
source_dir="${github_repo_location}/Data_analysis_pipeline/02_assembly_and_binning/enrichment_metagenomes"
fasta_files=($(find ${source_dir} -name "*.fasta" | grep "genomic_bins" | grep -v "checkm" | sort -h))

for fasta_file in ${fasta_files[@]}; do
    base_name=${fasta_file%.fasta}
    base_name=${base_name##*/}
    echo "[ $(date -u) ]: Linking '${base_name}'"
    ln -s ${fasta_file} ${base_name}.fa
done

# Get the curated bins
source_dir="${github_repo_location}/Data_analysis_pipeline/03_bin_curation/03_contig_ordering/ordered_genomes/final"
fna_files=($(find ${source_dir} -name "*.fna" | sort -h))

for fna_file in ${fna_files[@]}; do
    base_name=${fna_file%.fna}
    base_name=${base_name##*/}
    echo "[ $(date -u) ]: Linking '${base_name}'"
    ln -s ${fna_file} ${base_name}.fa
done

# Now MANUALLY delete the non-curated versions of the curated bins that were also copied!
```
All genome bins should now be in the `${github_repo_location}/Data_analysis_pipeline/05_bin_analysis/01_all_genome_bins` folder and have the extension `.fa`.


## Taxonomic classification via GTDBTk
All genome bins were taxonomically classified using the Genome Tree Database Toolkit.

Downloaded the database (only need to do the first time)  
Release 86, version 3
```bash
database_dir="${github_repo_location}/Data_analysis_pipeline/05_bin_analysis/02_gtdbtk_taxonomy/database"
mkdir -p ${download_dir}

# Download the DB
wget -O - https://data.ace.uq.edu.au/public/gtdbtk/release_86/gtdbtk.r86_v3_data.tar.gz | tar -xzf -
# Makes the `release86` directory
mv release86/* .
rm -r release86
```

Install GTDBTk `v0.2.2`  
```bash
# Create the conda env
conda create -n gtdbtk_0.2.2 -c bioconda gtdbtk=0.2.2

# Then symlink the DB contents to the conda env
conda activate gtdbtk_0.2.2
db_filepaths=($(find ${database_dir} -maxdepth 1 | sort -h | tail -n +2))

for db_filepath in ${db_filepaths[@]}; do
    echo ${db_filepath##*/}
    ln -s ${db_filepath} ${CONDA_PREFIX}/share/gtdbtk-0.2.2/db
done
# Now GTDB will be able to find the DB automatically
```
Use this environment via `conda activate gtdbtk_0.2.2`

You can then test the install if you'd like:
```bash
test_dir="${github_repo_location}/Data_analysis_pipeline/05_bin_analysis/02_gtdbtk_taxonomy/gtdbtk_test"
threads=10

mkdir -p ${test_dir}
cd ${test_dir}
gtdbtk test --out_dir . --cpus ${threads}
```

Then run the classifier (needs ~100 GB RAM!)
```bash
# Set user variables
input_dir="${github_repo_location}/Data_analysis_pipeline/05_bin_analysis/01_all_genome_bins"
output_dir="${github_repo_location}/Data_analysis_pipeline/05_bin_analysis/02_gtdbtk_taxonomy"
threads=10

mkdir -p ${output_dir}
cd ${output_dir}

gtdbtk classify_wf --genome_dir ${input_dir} --out_dir ${output_dir} -x fa --min_perc_aa 10 --prefix ELA111314_dRep_gtdbtk --cpus ${threads} --debug
```
The output files `ELA111314_dRep_gtdbtk.ar122.summary.tsv` and `ELA111314_dRep_gtdbtk.bac120.summary.tsv` will be used downstream in Figure 3.  
By using the `--debug` flag, I'm able to keep the ORF predictions around to use for the next step below.

## Custom gene scanning via MetAnnotate
Several genes of interest were searched for in the entire binned genome set:
- *rpoB* - housekeeping gene (RNA polymerase gene) useful as a taxonomic marker
- *cyc2* - candidate gene marker for iron oxidation used in this study
- *dsrA* - a gene marker for sulfide oxidation or dissimilatory sulfate reduction

HMMs for *rpoB* and *dsrA* were downloaded from the FunGene website:
```bash
download_dir="${github_repo_location}/Data_analysis_pipeline/05_bin_analysis/03_metannotate/genome_bins/hmm_files"
mkdir -p ${download_dir}
cd ${download_dir}

## Nov 2009 versions (# TODO - I don't see a way to specify the version number, so there is a chance these will be replaced by newer versions in the future)
# dsrA.hmm
curl -LOJ http://fungene.cme.msu.edu/hmm_download.spr?hmm_id=8

# rpoB.hmm
curl -LOJ http://fungene.cme.msu.edu/hmm_download.spr?hmm_id=31
```

The *cyc2* HMM was developed in this study and is available in Supplementary File S2 (`cyc2_all_references.hmm`; see also the `04_HMM_development` folder in `Data_analysis_pipeline`). Added both the *Chlorobia*-specific version and the general version. Will use the general version for the paper figures, but the specific version is interesting to test alongside it for reference.
```bash
cd ${download_dir}

cp "${github_repo_location}/Other/File_S2/cyc2_all_references.hmm" .
cp "${github_repo_location}/Other/File_S2/cyc2_Chlorobia.hmm" .
```

Got prodigal `v2.6.3` amino acid predictions from the GTDBTk intermediate files (from the above GTDBTk step)
```bash
input_dir="${github_repo_location}/Data_analysis_pipeline/05_bin_analysis/02_gtdbtk_taxonomy/marker_genes"
output_dir="${github_repo_location}/Data_analysis_pipeline/05_bin_analysis/03_metannotate/genome_bins/orf_files"

mkdir -p ${output_dir}
cd ${output_dir}

genome_bin_dirs=($(find ${input_dir} -maxdepth 1 -mindepth 1 -type d | sort -h))
echo "[ $(date -u) ]: Linking ${#genome_bin_dirs[@]} genome bin ORF files"

for genome_bin_dir in ${genome_bin_dirs[@]}; do

    genome_basename=${genome_bin_dir##*/}

    # Clean up the name to remove any odd characters, to make more compatible downstream e.g., with MetAnnotate
    genome_basename_clean=$(echo ${genome_basename} | awk '{ gsub("[^A-Za-z0-9]", "_"); print }')

    # Hard link to the new directory
    echo "[ $(date -u) ]: Linking ${genome_basename}"
    ln ${genome_bin_dir}/${genome_basename}_protein.faa ${output_dir}/${genome_basename_clean}.faa

done

echo "[ $(date -u) ]: Linking finished."
```

Downloaded the RefSeq protein database  
**SKIP** this step if you've already downloaded the database before
```bash
# User variables
orf_dir="${github_repo_location}/Data_analysis_pipeline/05_bin_analysis/03_metannotate/genome_bins/orf_files"
hmm_dir="${github_repo_location}/Data_analysis_pipeline/05_bin_analysis/03_metannotate/genome_bins/hmm_files"
output_dir="${github_repo_location}/Data_analysis_pipeline/05_bin_analysis/03_metannotate/genome_bins/metannotate"
download_dir="${github_repo_location}/Data_analysis_pipeline/05_bin_analysis/03_metannotate/database"

mkdir -p ${download_dir} ${output_dir}

# Enter the docker container (will auto-download if you don't have it installed)
enter-metannotate ${download_dir} ${orf_dir} ${hmm_dir} ${output_dir}

# Download the database (can take a while)
cd $METANNOTATE_DIR
sudo chown linuxbrew ../databases
bash refseq_installation.sh /home/linuxbrew/databases
sudo chown -R root:root ../databases
# The 'chown' commands temporarily make the output folder belong to the linuxbrew user inside the Docker container so that the user can run the Docker commands. Files are given back to you at the end.

# Leave the container
exit
```

Ran MetAnnotate on the predicted amino acid sequences
```bash
# User variables
# NOTE: CHANGE the database_dir path if you've already downloaded Refseq.fa somewhere else on your machine and want to use that instead
orf_dir="${github_repo_location}/Data_analysis_pipeline/05_bin_analysis/03_metannotate/genome_bins/orf_files"
hmm_dir="${github_repo_location}/Data_analysis_pipeline/05_bin_analysis/03_metannotate/genome_bins/hmm_files"
output_dir="${github_repo_location}/Data_analysis_pipeline/05_bin_analysis/03_metannotate/genome_bins/metannotate"
download_dir="${github_repo_location}/Data_analysis_pipeline/05_bin_analysis/03_metannotate/database"

mkdir -p ${output_dir}
cd ${output_dir}

# Enter the docker container (will auto-download if you don't have it installed)
enter-metannotate ${download_dir} ${orf_dir} ${hmm_dir} ${output_dir}

threads=10

ref_UID=$(stat -c "%u" /home/linuxbrew/output)
sudo chown -R linuxbrew /home/linuxbrew/output
echo ${threads} > MetAnnotate/concurrency.txt # set the number of threads
metannotate-wrapper-docker sequence orf_files hmm_files 2>&1 | tee output/metannotate_wrapper_docker.log
sudo chown -R $ref_UID /home/linuxbrew/output
# The 'chown' commands temporarily make the output folder belong to the linuxbrew user inside the Docker container so that the user can run the Docker commands. Files are given back to you at the end.

# Leave the docker container
exit
```

The output file `all_annotations`[random_code]`.tsv` shows the hits of the HMMs on all genomes, along with the e-value score. It was renamed to `metannotate_annotations_genome_bins.tsv` for clarity in this folder. This output file is used in the code for Figure 3 -- see the Figure 3 folder.

Note that MetAnnotate hung on the final annotation combining step on my machine, so I had to manually merge the annotation files from each individual HMM to make `metannotate_annotations_genome_bins.tsv`:
```
cd "${github_repo_location}/Data_analysis_pipeline/05_bin_analysis/03_metannotate/genome_bins/metannotate"
output_filename="metannotate_annotations_genome_bins.tsv"
hmm_names=(cyc2_Chlorobia cyc2_all_references dsrA rpoB)

# Initialize output file
# TODO - add similar check here to below (although if the match fails, it will still be picked up below...)
head -n 1 ${hmm_names[0]}*_all_annotations*.tsv > ${output_filename}

for hmm_name in ${hmm_names[@]}; do

    echo "[ $(date -u) ]: Adding annotations from '${hmm_name}'"

    # Confirm only one file matches the pattern
    if [ $(find . -name "${hmm_name}*_all_annotations*.tsv" | wc -l) != 1 ]; then

        echo "[ $(date -u) ]: ERROR: found multiple files matching the pattern '${hmm_name}*_all_annotations*.tsv'. Exiting..."
        exit 1

    fi

    # Merge
    tail -n +2 ${hmm_name}*_all_annotations*.tsv >> ${output_filename}

done
```

## Relative abundance profiling using read mapping
(This requires you to have run ATLAS. Otherwise, the output file from this analysis is provided in the repo.)  
First had to install (only for first use)
```bash
## Get the git repo
work_dir="${github_repo_location}/Data_analysis_pipeline/05_bin_analysis/04_read_mapping"
mkdir -p ${work_dir} && cd ${work_dir}

git clone https://github.com/jmtsuji/atlas-extensions.git
cd ${work_dir}/atlas-extensions
git checkout 5b14f12

## Make the conda env and print the versions
conda create -y -n bin_mapping_stats -c conda-forge -c bioconda -c r samtools bbmap r r-plyr r-dplyr r-getopt

## Finish install
conda activate bin_mapping_stats
cp calculate_bin_abundance_in_metagenome.sh calculate_coverage_stats.R aggregate_mapping_stats.R ${CONDA_PREFIX}

# Log the conda env contents
conda list -n bin_mapping_stats > conda_env.log
```

Then run the profiler
```bash
atlas_dir="${github_repo_location}/Data_analysis_pipeline/02_assembly_and_binning"
input_dir="${github_repo_location}/Data_analysis_pipeline/05_bin_analysis/01_all_genome_bins"
work_dir="${github_repo_location}/Data_analysis_pipeline/05_bin_analysis/04_read_mapping"
qc_link_dir=${work_dir}/QC_reads
threads=12
memory=10 # gigabytes

# Make links of all QC'ed reads
mkdir -p ${link_dir}
find ${atlas_dir} -mindepth 4 -maxdepth 4 -name "*_QC_*.fastq.gz" | sort -h | xargs -I {} ln -s {} ${qc_link_dir}

cd ${work_dir}
calculate_bin_abundance_in_metagenome.sh ${input_dir} ${qc_link_dir} output ${threads} ${memory} 2>&1 | tee calculate_bin_abundance_in_metagenome.log
# Started 180927 at ~2:52 AM EDT
```
See the main output file, `genome_bin_mapping_stats.tsv`, pre-included in the repo.

Also estimate the percentage of reads that assembled via the ATLAS read mapping files  
(This requires you to have run ATLAS. Otherwise, the output file from this analysis is provided in the repo.)

```bash
# Set variables
atlas_dir="${github_repo_location}/Data_analysis_pipeline/02_assembly_and_binning"
output_dir="${github_repo_location}/Data_analysis_pipeline/05_bin_analysis/04_read_mapping"
output_filepath="${output_dir}/assembled_read_stats.tsv"

# HARD-CODED locations of the ATLAS BAM files of interest for this analysis (it's tricky because of the co-assembly to do this automatically)
bam_files=("${atlas_dir}/lake_metagenomes/coassembly/CA-L227-2013/multi_mapping/L227-2013-6m.bam"
    "${atlas_dir}/lake_metagenomes/coassembly/CA-L227-2013/multi_mapping/L227-2013-8m.bam"
    "${atlas_dir}/lake_metagenomes/coassembly/CA-L227-2014/multi_mapping/L227-2014-6m.bam"
    "${atlas_dir}/lake_metagenomes/coassembly/CA-L227-2014/multi_mapping/L227-2014-8m.bam"
    "${atlas_dir}/lake_metagenomes/coassembly/CA-L442/multi_mapping/L442-2011-16-5m.bam"
    "${atlas_dir}/lake_metagenomes/coassembly/CA-L442/multi_mapping/L442-2014-15m.bam"
    "${atlas_dir}/enrichment_metagenomes/L227_S_6D/sequence_alignment/L227_S_6D.bam"
    "${atlas_dir}/enrichment_metagenomes/L304_S_6D/sequence_alignment/L304_S_6D.bam")

# Initialize the output file
mkdir -p ${output_dir}
cd ${output_dir}
printf "metagenome\ttotal_reads\tmapped_assembled_reads\n" > ${output_filepath}

# Count the mapped reads
for bam_file in ${bam_files[@]}; do

    filename_base=${bam_file%.bam}
    filename_base=${filename_base##*/}

    echo "[ $(date -u) ]: Counting '${bam_file##*/}'"
    printf "${filename_base}\t" >> ${output_filepath}

    # Get the total reads
    total_reads=$(samtools view -c ${bam_file} >> ${output_filepath})
    printf "${total_reads}\t" >> ${output_filepath}

    # Get the mapped reads
    mapped_reads=$(samtools view -c -F 4 ${bam_file} >> ${output_filepath})
    printf "${mapped_reads}\n" >> ${output_filepath}

done
echo "[ $(date -u) ]: Done."
```
See output file, `assembled_read_stats.tsv`, in the repo folder.

These output files are used in Figure 3 -- see the Figure 3 folder for details.

## Relative abundance profiling of targeted genes in raw read data via MetAnnotate
This was done as a cross-comparison to the above MetAnnotate code.

The same three HMMs as mentioned earlier for MetAnnotate were used, along with one more for reference: *bchL* - pigment synthesis gene.
```bash
download_dir="${github_repo_location}/Data_analysis_pipeline/05_bin_analysis/03_metannotate/unassembled_reads/hmm_files"
mkdir -p ${download_dir}
cd ${download_dir}

# Get the HMMs used for the genome bins
cp ../../genome_bins/hmm_files/*.hmm .

# Get the bchL HMM
# This is a pain; you have to download the entire TIGRFAM database (~0.1 Gb) and then pull out the HMM of interest
# Using TIGRFAM release 15.0
mkdir -p ${download_dir}/../TIGRFAM_15_0
cd ${download_dir}/../TIGRFAM_15_0
wget -O - ftp://ftp.jcvi.org/pub/data/TIGRFAMs/TIGRFAMs_15.0_HMM.tar.gz | tar -xzf -
cp TIGR01281.HMM ../hmm_files/TIGR01281_bchL.hmm

cd ../hmm_files
rm -r ../TIGRFAM_15_0 # Or skip this step if you want to keep TIGRFAM on your server
```

This first requires FGS++ (commit `299cc18`) to be installed semi-manually
```bash
conda create -y -n FGS++ python=3
conda install -y -n FGS++ -c anaconda -c conda-forge meson ninja

# Get FGS++ source code
cd /tmp
git clone https://github.com/unipept/FragGeneScanPlusPlus.git
cd FragGeneScanPlusPlus
git checkout 299cc18

# Build
conda activate FGS++
meson build
ninja -C build

# Then copy built binaries to conda env
cp build/FGS++ ${CONDA_PREFIX}/bin
cp -r train ${CONDA_PREFIX}/bin

# Then deleted install folder.
cd ..
rm -rf FragGeneScanPlusPlus
```
Use via `conda activate FGS++`


Predict ORFs from raw metagenomic data
```bash
conda activate FGS++

raw_read_dir="${github_repo_location}/Data_analysis_pipeline/05_bin_analysis/04_read_mapping/QC_reads" # made above
orf_dir="${github_repo_location}/Data_analysis_pipeline/05_bin_analysis/03_metannotate/unassembled_reads/orf_files"
train_file_dir=""${CONDA_PREFIX}/bin/train"
threads=12
memory=50000 # in megabytes

mkdir -p ${orf_dir}
cd ${orf_dir}

fastq_files=($(find ${raw_read_dir} -type f -name "*.fastq.gz" | sort -h))

for fastq_file in ${fastq_files[@]}; do

    name_base=${fastq_file%.fastq.gz}
    name_base=${name_base##*/}
    echo "[ $(date -u) ]: Predicting ORFs for '${name_base}'"

    seqtk seq -A ${fastq_file} | \
        FGS++ -s stdin -o ${orf_dir}/${name_base}.frag -w 0 -r ${train_file_dir} \
        -t illumina_10 -p ${threads} -m ${memory} 2>&1 | tee ${orf_dir}/${name_base}.frag.log

done
```

Then run Metannotate
```bash
# User variables
# NOTE: CHANGE the database_dir path if you've already downloaded Refseq.fa somewhere else on your machine and want to use that instead
# If you haven't downloaded Refseq.fa yet, see the earlier usage of MetAnnotate in this file for how to do so
orf_dir="${github_repo_location}/Data_analysis_pipeline/05_bin_analysis/03_metannotate/unassembled_reads/orf_files"
hmm_dir="${github_repo_location}/Data_analysis_pipeline/05_bin_analysis/03_metannotate/unassembled_reads/hmm_files"
output_dir="${github_repo_location}/Data_analysis_pipeline/05_bin_analysis/03_metannotate/unassembled_reads/metannotate"
download_dir="${github_repo_location}/Data_analysis_pipeline/05_bin_analysis/03_metannotate/database"

mkdir -p ${output_dir}
cd ${output_dir}

# Enter the docker container (will auto-download if you don't have it installed)
enter-metannotate ${download_dir} ${orf_dir} ${hmm_dir} ${output_dir}

threads=20

ref_UID=$(stat -c "%u" /home/linuxbrew/output)
sudo chown -R linuxbrew /home/linuxbrew/output
echo ${threads} > MetAnnotate/concurrency.txt # set the number of threads
metannotate-wrapper-docker sequence orf_files hmm_files 2>&1 | tee output/metannotate_wrapper_docker.log
sudo chown -R $ref_UID /home/linuxbrew/output
# The 'chown' commands temporarily make the output folder belong to the linuxbrew user inside the Docker container so that the user can run the Docker commands. Files are given back to you at the end.

# Leave the docker container
exit
```

The output file `all_annotations`[random_code]`.tsv` shows the hits of the HMMs on all datsets, along with the e-value score. This was renamed (and gzipped) to `metannotate_annotations_unassembled_reads.tsv.gz` for clarity. This file is used in the code for Supplementary Figure S3. Because of its size, it is available for download on Zenodo -- see the Supplementary Figure S3 folder.

